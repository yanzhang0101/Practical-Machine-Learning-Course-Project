---
title: "Maching Learning Course Project"
author: "Yan Zhang"
date: "January 1, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gridExtra)
library(plyr)
library(ggplot2)
```

## Executive Summary

In this analysis we used Weight Lifting Exercise Dataset to develop predictive model(s) and attempt to predict the "classe" variable from the other variables. 

The pml-training data set was further partitioned into training and validation, and the training set was used to develop the model while the validation set was used to evaluate the resulting model.

The final model was a random forrest model with out of sample accuracy of 99.7% on the validation set.

## Explorative Analysis

Initial explorative analysis indicates that a large number of the variables has mostly NA values (13464 out of 13737 observations), hence these columns will provide minimum information. We produced a reduced data set (training2) with those columns removed, and examed the output variable to verify that the 13464 observations are not significantly differnt from the overall training set.

### Read the data and pre-process NA's, then partition the data into training and testing sets
```{r echo=FALSE}
library(caret)

dat = read.csv("pml-training.csv", header = T, na.strings = c("", " ", "NA", "#DIV/0!"))
set.seed(12345)
inTrain = createDataPartition(dat$classe, p=0.7, list = FALSE)

training = dat[ inTrain,]
validation = dat[-inTrain,]

```

### Exploratory analysis on the Training data set

```{r echo=TRUE}
NaR <- rowSums(is.na(training))
NaR.1 <- NaR>90
NaR.2 <- NaR<=90

par(mfrow=c(1,3))
plot(training$classe, main = "Full Training")
plot(training[NaR.1,]$classe, main = "Observations with NA's")
plot(training[NaR.2,]$classe, main = "Osbservations without NA's")

```

### Produce reduced (columns) data set by removing the unique id "X" and the columns that have over 95% of volume as "NA"

```{r echo=FALSE}
NaC <- colSums(is.na(training))
NaC.keep <- NaC<13000
NaC.keep[1] <- FALSE
NaC.keep[3:5] <- FALSE

training2 <- training[, NaC.keep]
dim(training2)
dim(training)

```

## Fitting the models on the reduced dataset

Fit three models:
* Decision Tree
* Random Forrest
* Combined Model

```{r echo=TRUE, message=FALSE, cache=TRUE, results=FALSE}

mod1 <- train(classe~., data = training2, method = "rpart")
mod2 <- train(classe~., data = training2, method = "rf")

pred1 <- predict(mod1, training)
pred2 <- predict(mod2, training)

predDF <- data.frame(pred1, pred2, classe=training$classe)
combMod <- train(classe~., method = "rf", data = predDF)

pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predVDF <- data.frame(pred1=pred1V, pred2=pred2V)
combPredV <- predict(combMod, predVDF)


```


## Model Comparison and selection

```{r echo=FALSE, results=TRUE}
table(pred1V, validation$classe)
table(pred2V, validation$classe)
table(combPredV, validation$classe)

validation$pred1Right <- pred1V==validation$classe
validation$pred2Right <- pred2V==validation$classe
validation$predComRight <- combPredV==validation$classe

colSums(validation[,161:163])
colSums(validation[,161:163])/nrow(validation)

```

## Conclusion

The random forrest model does just as well as the combined model, but requires significantly less computing power, hence is the chosen final model. 

The accurate classification rate on the validation data set for the model is 99.7%, which should hold true for out of sample prediction. I expecte the out of sample error to be less than 1%. 


